---
title: 'MEAPS \& gravity: estimates at La Rochelle'
author:
- name: Maxime Parodi
  email: maxime.parodi@sciencespo.fr
  affiliation: OFCE, Sciences Po Paris
  affiliation-url: http://www.ofce.fr
  orcid: 0009-0008-2543-5234
- name: Xavier Timbeau
  email: xavier.timbeau@sciencespo.fr
  affiliation: OFCE, Ecole Urbaine, Sciences Po Paris
  affiliation-url: http://www.ofce.fr
  orcid: 0000-0002-6198-5953
wp: 4
annee: 2024
date: 02/02/2024
date-modified: today
lang: fr
format:
  wp-html: default
  wp-pdf:
    output-file: MEAPS et gravitaire estimations a La Rochelle.pdf
citation:
  type: article-journal
  container-title: Document de travail de l'OFCE n°2024-4
  url: https://preview.meaps.fr
abstract: L’estimation de modèles gravitaires est faite usuellement en utilisant les
  moindres carrés ordinaires. Nous montrons que la distribution des flux de navetteurs
  correspond mal à un modèle où l’erreur est log-normale. La représentation par une
  processus de Poisson, que l’on peut estimer par glm est plus apropriée. A partir
  de la log-vraisemblance, on remarque que l’estimation par glm est équivalente à
  la minimisation de l’entropie relative de Kullback-Leibler, ce qui permet l’estimation
  par minimisaiton non linéaire d’une famille plus grande de modèles, dont MEAPS.
  La discussion des effets fixes ou aléatoires permet de comprendre en quoi la modélisation
  respecte ou non les contraintes aux marges du problème. On montre que les modèles
  qui respectent ces marges peuvent être employés hors échantillon pour prédire les
  flux de navetteurs. enfin, en utilisant une information infracommunale on peut améliorer
  la qualité de l’ajustement de MEAPS et produire une intrapolation des flux à une
  résolution plus importante que celle des données de flux issues du recensement.
  <br> `r  wordcountaddin::word_count('larochelle.qmd')` mots.
keywords:
- modèle gravitaire
- modèle radiatif
- mobilités
- glm
- poisson
- entropie relative
bibliography:
- references_meaps.bib
---

```{r init, include=FALSE}
source("R/rinit.r")
```

Comparing a model with data is a crucial step in understanding how it works and assessing its relevance. Here we explore the ability of the MEAPS model to reproduce mobility flows by comparing it with the gravity model. The question is which model is best able to reproduce the observed flow data, i.e. the MOBPRO data, while introducing a minimum of parameters, for the sake of parsimony and generality. Furthermore, this ability to explain the data must be based on a theoretical foundation that is as explicit as possible, which is a prerequisite for being able to interpret the estimated parameters.

The parametrisation of the model, the choice of statistical model, or the metric to be minimised to determine the parameters are all points that need to be clarified, depending on the nature of the data available, but also on what we believe to be the process that generated them. This discussion is crucial because it can lead to very different estimates, and it is necessary to explain why one is preferred to the other. It is also important for diagnosing the quality of the modelling revealed by the data we use, and for feeding both the modelling process and our understanding of the data.

The starting point is the estimation of a gravity model using ordinary least squares (OLS). This is what is usually done in the literature (@josselin2020 for the PACA region in France, for example, @lenormand2016, @masucci2013 for other conurbations). But this approach deserves to be developed further.

## Specifications of the models to be estimated

We propose to use the relative entropy (or information criterion or log likelihood of a multinomial distribution) as the objective function to be minimised. An ordinary least squares regression weighted by the flows (i.e. the explained variable) gives a result closer to the relative entropy than the unweighted mean square error. We then carry out non-linear estimations using mainly relative entropy as the objective function (other objective functions are presented for comparison). This approach makes it possible to estimate gravity models with a simple constraint (the constant $c$ of the @eq-gravity is no longer estimated and is replaced by a vector $c_i$which ensures compliance with the in-line constraint\[\^1\]) and double constraint (both the row and column constraints are satisfied using, for example, the Furness procedure). Using the same non-linear estimation procedure with relative entropy as the objective function, we also estimate*MEAPS* in order to allow parametrisation.

\[\^1\] Inspire: INfrastructure for SPatial InfoRmation in Europe has been a directive for the production of spatial data since 2007. Inspire defines a grid and its harmonised projection system. This is what follows INSEE in the dissemination of gridded data. See<https://inspire-geoportal.ec.europa.eu> for the definition of the grid and data sets.

Finally, we apply these estimates using sub-communal information to show that this information can increase the explanatory power of the models, in particular of *MEAPS*. The intuition is that sub-municipal information allows finer parameterisation than on the basis of municipal information. Although the variable being explained (flows) is known at the municipal level, the injection of sub-municipal information increases the explanatory power of the models used, particularly for *MEAPS*.

### The standard gravity model: lognormal errors

The gravity model is usually estimated using linear regression [@masucci2013 ; @lenormand2016; @josselin2020] by ordinary least squares. The following model is the one estimated, where the observed flows$f_{ij}$ are the explained variable and jobs $e$ distributed in $J$ spatial units, the working population $n$ distributed in $I$spatial units and the distance matrix[^1]$[d_{ij}]$ are the explanatory factors:

[^1]: To simplify the exposure, we have chosen a special "distance" function, set by$\delta$ called a power function with the form $1/d^\delta$. Alternatives are possible, such as the exponential function, written as $e^{-d/\delta}$ or any other function of distance, possibly with more than one parameter. Various metrics can be used to analyse distances. These may be distance as the crow flies, distance travelled by road networks or journey time - which allows public transport to be included. We can also include a generalised cost of transport, derived for example from a discrete choice model, which allows us to take into account concepts such as individual preferences for a particular mode of transport (implying different speeds and therefore different times) or the comfort felt by a mode of transport, whether during the journey or through the security it provides in the slight uncertainty of its realisation. For the municipal analysis, we will use the Euclidean distance as the crow flies. For sub-municipal analyses, we will use distances and journey times via the road or public transport networks in each municipality, using different modes of transport.

$$
log(f_{ij}) = \alpha \times log(n_i) + \beta \times log(e_j) - \delta \times d_{ij} + c + \varepsilon_{ij}
$$ {#eq-gravitaire}

$$
\varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
$$

Thus written, the gravity model only respects the separability property if the coefficients $\alpha$ and $\beta$ are equal to 1. When $\alpha$ is not equal to 1, separate a group of $n_i$ into two sub-groups, for which the distances are unchanged, leads to the projection of flows whose sum differs from the flow calculated for the two sub-groups combined. Symmetrically, $\beta$ different from 1 implies the same non-separability when separating a group $e_j$ into two. This property is necessary for the use of the model. For example, the granularity of the spatial aggregation should not alter the predicted flows too much, especially when this aggregation is fine enough so that the relative distances do not change too much. The separation of jobs into jobs by sector, or of individuals according to socio-economic characteristics or preferences, is another example of a transformation to which the model must be robust. If jobs are separated into two sectors and behaviour remains unchanged along this separation, the flows must add up if the model is to be consistent.

If the estimation leads to $\alpha\approx\beta\approx1$the separability property will be (approximately) respected. As we shall see in the estimates, and as is generally the case in the literature [for example @josselin2020 for the PACA region] region, as a general rule, OLS estimates of the gravity model (i.e. with lognormal errors) give$\alpha$ and $\beta$ significantly less than 1. Failure to comply with this separability property reduces the model's scope and likelihood.

### Which generating process: Gaussian or poisson?

The log-linear gravity model estimated by OLS assumes multiplicative noise and a generating process whose errors compensate each other. However, flows are more like the result of a count than the process implicit in the OLS model. Flows should therefore no longer be considered as errors that compensate for each other around a deterministic model, but as the result of a contingency table and independent events whose accumulation can be observed.

Several representations are possible. A first approach is to consider the result as that of a random draw of $n$ balls of different colours (the $i$) and placed in $j$ domes at random. This is a multinomial distribution where each cell in the table has a probability of $\pi_{ij}$ and the frequencies are $f_{ij} = n \times \pi_{ij}$ and where $n=\sum f_{ij}$.

An alternative, very often used in particular in generalized linear models, is to consider the contingency table as produced by mulitvariate Poisson processes [@aitchison1989, @agresti2002].

One of the properties that differentiates these two broad categories of models (multinomial and Poisson versus log normal) is in the vicinity of low values. Intuitively, the variance of a Poisson process is the expectation of the process. For a lognormal process, the variance is a function of the standard deviation and the mean of the log noise, and tends towards a strictly positive value when the mean of the log noise tends towards 0.[^2]. This property implies that relative errors for small values in a log-normal representation have a larger variance than in a Poisson process.[^3].

[^2]: Si$log(\varepsilon) \sim \mathcal N ( \mu, \sigma^2)$ then $Var(\varepsilon) = (e^{\sigma^2}-1)e^{2\mu+\sigma^2}$.

[^3]: More generally, the family`quasi,` in the `glm` makes it possible to specify a link between variance and mean and thus to define behaviour in the vicinity of 0 that is not constant (log linear), related to the mean (Poisson) but in the square of the mean (Bernouilli) or in the cube of the mean. The greater the dependence, the faster the variance tends towards 0.

Another way of appreciating the difference in approach between the lognormal model and the contingency table is through the log-likelihood. With one constant (independent of the parameters to be estimated), the log-likelihood for the log-linear model is the mean square error (*msre*), noting $\hat f$ the flow predicted by the model and $f$ the observation :

$$
msre = \sum_i {({log(f}_{ij}) - log(\hat f_{ij}))^2 }
$$ {#eq-msre}

For the Poisson process, keeping the same notation, the probability of observations knowing the model is :

$$ P(\\hat f_{ij}\= f_{ij}) = \\frac{e^{-\\hat f_{ij}}times \\hat f_{ij}^{f_{ij}}}{f_{ij}!} $$ {#eq-poisson}

The Poisson parameter matrix $[\hat f_{ij}]$ can then be estimated from a log-linear model, which can be easily implemented by `glm`. This is the approach that @flowerdew1982 applies to mobility flows. The gravity model can then be written simply as :

$$ log(\\hat f_°{ij}) = \\alpha \\times log(n_i) + \\beta \\times log(e_j) - \\delta \\times d_{ij} +c $$ {#eq-poissongrav}

This gives the log-likelihood, where $n=\sum f_{ij}$and approximating the factorial by Stirling's formula [@agresti2002] :

$$ \\mathcal{L} = -\\hat n + \\sum_°{ij}{f_{ij} log(\\hat f_{ij})} - \\sum_{ij}{f_{ij}log(f_{ij})}$$ {#eq-lvpoisson}

In this expression, when $n$ is known (and therefore $\hat n = n$), the Poisson process is a multinomial distribution the log-likelihood is simplified by eliminating $\hat n$. Thus, the log-likelihood for a multinomial process is :

$$
\mathcal L = \sum_{ij} f_{ij} log(\hat f_{ij} /n)
$$ {#eq-llmulti}

The expression of the log likelihood is then (with one constant independent of the parameters) equal to the information gain or relative entropy criterion [@kullback1951] :

$$ I(f_{ij}\\hat f_{ij}) = \\frac 1 n \\sum_i {f_{ij} \\times (log(\\hat f_{ij}) - log(f_{ij}))} $$ {#eq-gaininfor}

As noted by @flowerdew1982, @sen1995 and @griffith2016, ordinary least squares is based on minimising the mean square error, but mobility flows between pairs of municipalities do not follow this distribution (even when corrected by the deterministic part) and the estimate is biased. Thus, a few origin-distribution pairs concentrate the vast majority of flows, while a large number of origin-destination pairs represent small flows and a very small cumulative share of total flows. In the case of La Rochelle and the surrounding area, La Rochelle-La Rochelle flows account for almost 20% of total flows, and the 40 largest flows (out of 2,125) account for more than 50% of all flows. The assumption of a multinomial or Poisson generating process means that the possibility of small flows, associated with small Poisson parameters, is correctly taken into account. The lognormal model gives too much importance to small flows, which is all the more problematic given that, due to the nature of the problem, there are very many small flows. As a result, the difference between the metrics (mean square error versus relative entropy) will be all the greater the further the data we are using is from a uniform or normal distribution. This intuition will be illustrated by comparing the observed/predicted deviations of the different models we have estimated. As explained in [@agresti2002 pp.146-148] an alternative to the mean square error is to use a log-linear regression of the type @eq-gravitational but weighted by the flows (i.e. the anti-log of the variable explained). In this case, the objective function to be minimised is :

$$ msre_w = \\sum_i {  f_{ij} \\times (log(f_{ij}) - log(\\hat f_{ij}))^2 } $$ {#eq-msrew}

We will see that, in addition to the square, there is a difference between this criterion and the relative entropy criterion which relates to whether or not we know the total sum of the flows or the sums in rows or columns. As illustrated below for La Rochelle, the weighted criterion produces results close to those obtained by `glm` and Poisson.

### Row and column constraints

The difference between the multinomial model and the Poisson model lies in the nature of the information available. In the multinomial model, the total number of individuals is fixed, whereas in the Poisson model it is random. When, in the Poisson model, the counts in each cell of the contingency table are converted into probabilities, these probabilities follow a multinomial distribution. If we know not only the number of individuals, but also the margins of the contingency table (i.e. the number of individuals in each row, which means that each individual holds a job, and the number of jobs in each column, which means that each job is held), then the underlying distribution is multivariate hypergeometric.\[\^5\](\$f(f\\\_{ij}). Unfortunately, it can only be used for very small dimensions, for example in Fisher's exact test. [@agresti2002].

The simple formulation of the gravity model is unconstrained in both rows and columns. The expectation of the sum $\sum_j \hat f_{ij}$ is different from the $\sum_j {f_{ij}}$ which is generally lower because of the convexity of the function $log$. To approximate these constraints, we need to introduce additional parameters, in the form of fixed or random effects. The $I$ constraints (also called simple constraints, or flow production constraints) are represented by replacing the constant $c$ by a vector $a_i$ vector (of size $I$) :

$$
\begin{aligned}
log(f_{ij}) = & \alpha \times log(n_i) + \beta \times log(e_j) - \delta \times log(d_{ij}) \ & + log(a_i) + \varepsilon_{ij}
\end{aligned}
$$ {#eq-grav_sc}

$$
\varepsilon_{ij} \sim \mathcal{N}(0,\sigma^2)
$$

It can also be defined as explicitly respecting the $I$ line constraints, and therefore taking into account the convexity of the function $log$ as written in @eq-algsc. Estimation can no longer be done directly by linear regression, since the vector $a_i$ vector depends both on the estimate of $\alpha$ and that of $\beta$. They can be estimated by an iterative procedure or a non-linear minimisation, whose objective function can be the $msre$ by analogy with OLS, the relative entropy by analogy with a `glm` fish or the $msre_w$ to approximate the former.

$$
a_i = \frac{n_i}{\sum_j {\frac{n_i ^ \alpha \times e_j ^ \beta} { d_{ij}^{\delta} }}} = \frac{n_i ^ {1-\alpha}}{\sum_j { e_j ^ \beta / d_{ij}^{\delta} }}
$$ {#eq-algsc}

Whether the effect is fixed or random, as soon as we introduce the vector $a_i$ into the model, it is no longer possible to identify $\alpha$ and for $\alpha$ any value, the $a_i$ being a function of $\alpha$. However, in order to respect the separability property, it is necessary to choose $\alpha=1$. By the expression @eq-algsc, compliance with the online constraints necessarily leads to $\alpha=1$. This is because, $log(a_i)$ can be written as the sum of $(1-\alpha)log(n_i)$ and another term which depends only on $\beta$, $e_j$, $d_{ij}$ and $\delta$. The initial equation then reduces to a term in $log(n_i)$. In the case of compliance with line constraints, the elasticity of flows to assets is necessarily unitary.

Compliance with the $J$ column constraints (also called attraction constraints) is achieved in a similar way, by introducing a vector $b_j$. By the same reasoning, $\beta$ can no longer be estimated and will be set to 1, to respect the separability constraint. Double compliance with the row and column constraints can be achieved using the Furness procedure [@de2011modelling ; @griffith2016] by defining $a_i$ and $b_j$ as follows: $$
a_i = \frac{n_i}{\sum_j {{b_j n_i ^ \alpha e_j ^ \beta} / { d_{ij}^{\delta} }}} = \frac{n_i ^ {1-\alpha}}{\sum_j { b_j e_j ^ \beta / d_{ij}^{\delta} }}
$$ {#eq-algdc1} $$
b_j = \frac{e_j}{\sum_i { {a_i n_i ^ \alpha e_j ^ \beta} / { d_{ij}^{\delta} }}} = \frac{e_j ^ {1-\beta}}{\sum_i {a_i n_i ^ \alpha / d_{ij}^{\delta} }}
$$ {#eq-algdc2}

This last model, by Furness, can be estimated by non-linear optimisation without great difficulty, as the Furness procedure converges rapidly. By noting $\hat a_i = a_i/n_i^{1-\alpha}$ and $\hat b_j= b_j/e_j^{1-\beta}$ we have :

$$
\hat a_i = \frac{1}{\sum_j { \hat b_j e_j/ d_{ij}^{\delta} }}
$$ {#eq-algdchat1}

$$
\hat b_j = \frac{1}{\sum_i { \hat a_i n_i/ d_{ij}^{\delta} }}
$$ {#eq-algdchat2}

These last two equations show that in the case of the double constraint, the parameters $\alpha$ and $\beta$ disappear and the gravity model is reduced to the following equation :

$$
log(f_{ij}) = log(n_i) + log(e_j) - \delta \times d_{ij} + log(\hat a_i) + log(\hat b_j) + \varepsilon_{ij}
$$ {#eq-gravfurness}

The elasticity of each flow $f_{ij}$ to jobs or residents is necessarily unitary. This property introduces a subtle difference with the fixed or random effects formulation in which the parameters $\alpha$ and $\beta$ can be fixed at any value, the $a_i$ and $b_j$ adjust accordingly. This means that in the fixed/random effects formulation, the model is agnostic as to the value of the derivatives to the increase in the "active" or "employment" mass. The estimate of $\delta$ is made by determining for each value of $\delta$ the flows, to which Furness is applied, and therefore by calculating the $a_i(\delta)$ and $b_j(\delta)$. We then calculate the objective function $\mathcal{L}$ for these flows ($\mathcal{L}$ being the mean square error or the other objective functions) :

$$
\hat \delta = \underset{\delta}{\mathrm{argmax}} , \mathcal{L}(f_{ij}(\delta))$$ {#eq-argmax}

Whether we consider the model constrained by Furness or the model augmented by fixed or random effects, we are faced with a difficulty of interpretation. Regardless of how they are calculated, the coefficients $a_i$ and $b_j$ act as modifiers of the masses at origin or destination. The deterministic part of the gravity model can be written as @eq-gravaibj, which shows what these two coefficients correspond to. In order to work, the gravity model requires us to 'cheat' on the masses, which is not the case with the 'gravity' interpretation.

$$
\hat f_{ij} = f_0\frac{(a_i n_i)\times (b_j e_j)}{d_{ij}^\delta}
$$ {#eq-gravaibj}

The solution and interpretation proposed by @fotheringham1983 are more interesting. His point is that the gravity model lacks a variable describing the neighbourhood or environment of each location of interest. According to his interpretation, concentrations (of jobs located close to each other) could increase attractiveness if there is an agglomeration effect or, on the contrary, reduce it if there is a congestion effect or a search for solitude. However, his approach does not resolve the issue of row and column constraints, and simply assumes that the problem will be less acute once competition between opportunities is taken into account.

::: {#tip-entropie .callout-tip collapse="true"}
## Kullback-Leibler deviance and relative entropy

Deviance is used to define the goodness of fit of a model. $M$ by generalising the sum of squared errors. It consists of subtracting from the log likelihood of the saturated model (i.e. with as many parameters as observations) the log likelihood of the estimated model, where $y$ are the observations and $\hat y_M$ are the predictions based on the model $M$ :

$$
D(y, \hat y_M) = 2(log(P(y|M_{saturé}) - log(P(y|M))
$$

Deviance is not a distance, because it is not symmetrical and does not satisfy the triangle inequality.

This deviance can be normalised using the so-called null model, i.e. with a single parameter for the constant. We then have :

$$
R^2_{dev} = 1 - D(y, \hat y_M)/D(y, \hat y _{null})
$$

For a linear model, the deviance is the sum of the squared errors ( $\sum(y-\hat y)^2)$ ), and $R^2_{dev} = R^2$ which justifies the notation.

In the case of distributions ( $\sum p = 1$, $\sum q=1$), we can define a similar criterion based on the relative entropy of Kullback-Leibler [@kullback1951]. Relative entropy is defined for two probability distributions$p$ and $q$ as follows in the discrete case:

$$
KL(p,q) = \sum_{i}p_i \times log(p_i/q_i)
$$

In the context of information theory, it can be interpreted as the relative amount of additional information required to express a given situation. $q$ from $p$. Following @colincameron1997 we can construct a measure of goodness of fit $R_{KL}^2$ as follows, where $\hat{q}$ and $q_0$ are the estimated and reference distributions, respectively, which are compared with $p$ :

$$
R_{KL}^2 = 1 - \frac{KL(p,\hat{q})}{KL(p, q_0)}
$$

If the reference distribution is chosen as a uniform distribution, by analogy with the calculation of the variance in a $R^2$ where we regress on a constant. We write :

$$
\begin{aligned}
KL_u(p,q_{ref}) &{}= \sum_{i}p_i \times log(p_i/unif) \&{}= \sum_i p_i \times log(p_i) - log(N)
\end{aligned}
$$

This is simply the entropy of the distribution $p$ with one constant ($N$ is the total number of active residents or jobs). For very specific distributions, the adjustment coefficient defined in this way can have negative values or values greater than 1.

If we know the margins of the contingency table (number of working people in the origins $i$ and number of jobs in the destinations $j$), we can use as a reference not the uniform distribution but an independent distribution.

$$
KL_i(p,q_{ref}) = \sum_{ij}p_{ij} \times [log(p_{ij}) -log( \frac{n_i \times e_j}{N^2})]
$$

Based on these two references $R^2_{KLu}$ and $R^2_{KLi}$. The 2 $R^2_{KL}$ do not require knowledge of the likelihood and therefore of the underlying model. They coincide with the deviance when the model is a multinomial distribution.
:::

### Extension of MEAPS with *odds ratios* {#sec-odds}

To enable a more detailed specification, i.e. by adding parameters, from *MEAPS* we introduce for each pair (*i*, *j*) a parameter which modifies the probability of absorption of the individual *i* by the job *j*. We define $c_{abs}$ as the chance of absorption, defined as $c_{abs} = p_{abs}/(1-p_{abs})$ . In the *MEAPS* presented above, this chance of absorption is identical for all the jobs considered by an individual and depends only on the probability of flight. A simple way of injecting information into the model then consists of modifying this chance of absorption according to the individuals and the jobs they consider. Modifications to absorption probabilities can then be parameterised by *odds ratios* (relative odds ratios) $\omicron_{ij}$ so that the new chance of absorption from i to j is equal to $\tilde{c}_{abs,ij} = \omicron_{ij} \times c_{abs}$. L'*odds-ratio* $\omicron_{ij}$ is a parameter between $0$ and $+\infty$ and *i* and *j* index the departure and arrival municipalities. The new absorption probability is then written using the reference absorption probability and the*odds ratio* as follows:

$$ \\tilde{p}_{abs,ij} = \\frac{c_{abs} \\times \\omicron_°{ij}} {1+c_{abs} \\times \\omicron_{ij}} $$

An initial calibration strategy for *MEAPS* consists of calculating as many*odds ratios* so as to reproduce the aggregate flows of @MOBPRO as faithfully as possible. This method leads to saturation of the model, since we estimate a number of parameters close to or equal to the number of degrees of freedom imposed by @MOBPRO. This learning strategy is similar to that used in *machine learning* because of the multiplication of the number of parameters to be estimated. The limitation of this approach is over-fitting (*overfitting*) it induces. This is usually corrected by adding a penalty to the complexity of the model within the optimisation function. This can also be done by *pruning* by eliminating *a posteriori* parameters whose contribution to explaining the data is below a threshold.

The parameters resulting from this approach contain information that can then be exploited. The *odds ratios* can be interpreted relatively simply: odds ratios greater than 1 indicate that the corresponding career mobility flows are more frequent than predicted by the reference model; and vice versa for odds ratios less than 1. *odds ratios* ratios below 1.

A second strategy is non-linear estimation. We choose a structure for the *odds ratios* by parameterising them with one of the available data (for example $\omicron_{ij)} = O(d_{ij})$ the function $O$ function being parameterised by one or more parameters $\theta$. By defining an objective function (for example, the relative entropy of Kullback Leibler), we can estimate $\theta$ :

$$
\hat \theta = \underset{\theta}{\mathrm{argmin}} , \mathcal{L}(f^{meaps}_{ij}(O(d_{ij},\theta)))
$$

## Data at municipal level

### Job mobility

The main data we use comes from the detailed census file. We start with individual data, with location information at the commune/borough level for residence and employment. The data we use is taken from the detailed file on professional mobility for 2020.\[\^6\] available on the[INSEE website](https://www.insee.fr/fr/statistiques/7637844?sommaire=7637890&q=mobilites+professionnelles). We select the individuals belonging to our area of interest: the La Rochelle-Aunis SCoT for the estimate, and a selection of other SCoTs for the test, as illustrated in @fig-scots.

\[\^6\] The year 2020 was marked by the COVID. Apart from the difficulties in following the sample design, it can be estimated that the analysis of mobility flows is not disrupted to the first order. In fact, the census focuses on identifying the usual place of work and residence. The confinements have probably limited job changes, but the postponement of part of the field surveys may partly compensate for this job-fixing effect.

```{r}
#| label: fig-scots
#| fig-cap: carte des SCoT
#| fig-asp: 1

bd_read("carte_scot") +
  theme_ofce_void(panel.grid = element_blank())+
  labs(
    caption= "*Source* : data.gouv.fr, Schéma de cohérence territoriale (SCoT) - données SuDocUH -<br>Dernier état des lieux annuel au 31 décembre 2023")
```

The @tbl-scot gives some descriptive statistics for the different samples. In particular, column QQ plot indicates that the log flows are not normally distributed, with a large mass at the end of the distribution, which is consistent with a Poisson distribution of flows.

```{r scot_data}
scot_data <- bd_read("scot_data")
tbl_scot <- bd_read("tbl_scot")
```

::: {#tbl-scot}
::: {.content-visible when-format="html"}
```{r}
if(knitr::is_html_output())
  tbl_scot
```
:::

::: {.content-visible when-format="pdf" layout-nrow="2"}
```{r}
  t1 <- scot_data |> 
    select(scot, nflux, N, nCOMMUNE, nDCLT, dist, dist_sd) |> 
    gt() |> 
    fmt_number(columns = -c(scot, nflux, N, nCOMMUNE, nDCLT), n_sigfig = 3) |>
    fmt_integer(N, sep = "", suffixing = "k") |> 
    fmt_integer(columns = c(nflux, nCOMMUNE, nDCLT), sep="") |> 
    cols_label(scot = "", nflux = "flux", N = "actifs", nCOMMUNE = "origines", nDCLT = "destinations") |> 
    tab_spanner(label = "Nombre", columns = c(N, nflux, nCOMMUNE, nDCLT)) |> 
    tab_spanner(columns = c(dist, dist_sd), label = md("distances (km)")) |> 
    cols_label(dist = "moyenne", dist_sd = "écart type") |> 
    cols_align("left", scot) |> 
    tab_style(style = cell_borders(sides = c("bottom"), color = "grey"), locations = cells_body(rows = 1))
  

  t2 <- scot_data |>
    select(scot, mb, mb_sd, mb_s001, mbl, mbl_sd) |>
    gt() |>
    fmt_number(columns = -c(scot), n_sigfig = 3) |>
    fmt_percent(columns= c(mb_s001), decimals = 0) |>
    cols_label(scot = "") |>
    tab_spanner(columns = c(mb, mb_sd, mb_s001), label = md("f~ij~")) |>
    cols_label(mb = "moyenne", mb_sd = "écart type", mb_s001 = "part 1%") |>
    cols_label(mbl = "moyenne", mbl_sd = "écart type") |>
    tab_spanner(columns = c(mbl, mbl_sd), label = md("log~10~(f~ij~)")) |>
    cols_align("left", scot) |>
    tab_style(style = cell_borders(sides = c("bottom"), color = "grey"), locations = cells_body(rows = 1))
  
  t1
  t2
```
:::

Description of estimation and test samples
:::

In order to produce confidence intervals, we resample the estimation data (*bootstrap*). We use the detailed census file, drawing the individuals in the area concerned with a discount, with the probability of their weight in the sample divided by the sum of the weights. For individuals with weights close to 5, linked to the census sampling plan for municipalities with fewer than 10,000 inhabitants, we break them down into 5 individuals with weights divided by 5. This avoids accumulations around multiples of 5 in the resampled distribution.

Each sample "*boostrapé*"sample has the same number of individuals, with the same distribution, and individuals can be repeated, in accordance with the principle of *boostrap* (principle.)

We then associate a Euclidean distance as the crow flies between the centroids of the municipalities with each pair of municipalities of origin and destination. For movements from the same commune to the same commune, we define the distance as half the square root of the area. This is an approximation of the average value of the distance for points distributed randomly over a circle of the same area. In the sub-municipal part, this approximation is explicitly lifted by locating 200m residents and jobs on the tile and calculating the distances between all origin and destination pairs. All distances are expressed in kilometres.

The detailed census file contains many implicit 0s, i.e. pairs of municipalities for which there are no flows. When communes are very far apart, this is easy to understand. But when an area is analysed (such as the La Rochelle-Aunis SCoT), there are flows between municipalities that are further apart than others that are not linked by a flow. We have chosen to treat the absence of these flows as a structural 0, even though the alternative hypothesis of modelling these flows as almost zero can be defended.

## Communal" adjustments to gravity and flow models *MEAPS* {#sec-ajustcom}

### Gravity models by MCO or glm

The @tbl-gravcom gives the estimation results for various unconstrained gravity models (i.e. $\alpha$ and $\beta$ are estimated), constrained (i.e $\alpha=1$ and $\beta=1$), unweighted (standard metric) or weighted by level flows, estimated by `glm` with Poisson or quasi Poisson families, with or without fixed or random effects. The estimated form is @eq-grav or @eq-poissongrav, adding the coefficients $a_i$ and f$b_j$ when fixed or random effects are added. Regressions are performed either by `stat::lm` or by `stat::glm` in R, or by the package R `lme4` package for fixed or random effects. The table shows various metrics (see @tip-entropy for definitions), as well as the degrees of freedom of the residuals associated with each regression.

In all the regressions, the estimated parameters are largely significant. The unconstrained OLS regression (line 1 of @tbl-gravcom) shows coefficients of $\alpha$ and $\beta$ coefficients of less than 1, indicating clear non-separability. Constraining these 2 parameters significantly degrades the quality of the estimate (line 2), without altering the coefficient associated with the distance. For the PACA region, @josselin2020 estimates regressions close to that of line 1, with $R^2$ comparable to those for the La Rochelle perimeter. The coefficient of distance they estimate is between 0.9 and 1.4, well above that estimated for La Rochelle. This suggests that this coefficient incorporates more information than the simple effect of distance, and partly sums up the geographical information that is not introduced into this regression by any variable.

```{r}
#| label: tbl-gravcom
#| tbl-cap: Estimations communales, modèles gravitaires
bd_read("tbl_grav_glm")
```

As mentioned above, one of the problematic aspects of OLS regression is the treatment of large flows. This aspect is illustrated by the @fig-residusgravglm where observed versus estimated flows are represented. OLS regressions have difficulty estimating the largest flow (from La Rochelle to La Rochelle). Using the generalised linear model (`glm`) with Poisson processes resolves this issue. The @tbl-gravcom line 7 regression illustrates this point. The observed versus estimated graphs provide a graphical appreciation of the improved fit. The bias for large flows is smaller, as is the overall dispersion.

The coefficients $\alpha$ and $\beta$ are closer to 1, but for all that, the separability property is not respected, as illustrated by the constrained regression in line 8, for which the goodness of fit is weaker. Estimates by "quasiPoisson" models, in order to take account of any over-dispersion compared with the Poisson model, produce results identical to those by the Poisson model.

Flow-weighted regressions (lines 4 to 6), whose objective function is close to the criterion of information gain or relative entropy, lead to estimates very close to the models of coefficients $\alpha$ and $\beta$ coefficients. However, they remain below 1 and when they are constrained to 1 (line 5), the fit, as measured by the $R^2_{dev}$ deteriorates significantly. The estimated coefficient for distance depends fairly heavily on this assumption, except for the OLS estimate.

```{r}
#| label: fig-residusgravglm
#| fig-cap: Modèles gravitaires, Observés versus estimés
#| fig-asp: 1

flux.grav <- bd_read("flux.grav")
data <- flux.grav |>
  filter(dist == "euc", !str_detect(nom, "RE"), !str_detect(nom, "quasipoisson")) |> 
  mutate(method = factor(method), submethod = factor(submethod))
ggplot(data |> biscale::bi_class(method, submethod, dim=3))+
  geom_point(aes(x=mobpro, y=flux,
                 color = bi_class, size=sqrt(mobpro)), alpha=.5, 
             show.legend = FALSE, stroke = 0) +
  scale_size(range = c(0.25, 2.5))+
  scale_x_log10(name = "observé", limits=c(1, 60000), labels = c("1", "10", "1k", "50k"), breaks = c(1, 10, 1000, 50000))+
  scale_y_log10(name = "estimé", limits=c(1, 60000), labels = c("1", "10", "1k", "50k"), breaks = c(1, 10, 1000, 50000))+
  biscale::bi_scale_color(pal= "BlueYl", dim=3 ) +
  geom_abline(slope=1, linewidth = 0.1, color = "grey", linetype = "solid")+
  coord_equal()+
  facet_grid(rows = vars(submethod), cols = vars(method))+
  theme_ofce(base_size = 9, 
             strip.text = element_text(size = 8),
             panel.grid.major.x = element_line(color = "grey", linewidth = 0.1))+
  labs(caption="*Sources* : MOBPRO, MEAPS")
```

By adding fixed or random effects\[\^7\] effects for each origin and destination (lines 6 and 9), and therefore a large number of parameters, we improve the quality of the estimate and ensure compliance with the separability property by setting the coefficients$\alpha$ and $\beta$ à 1. The coefficient of distance (lines 3, 6 and 9) is higher and significantly different from those estimated without fixed effects (around 1.3 instead of around 1 in the unconstrained regressions (lines 1, 4 or 7)). As noted above, while fixed effects improve the quality of the estimation, this is to the detriment of the initial spirit of the gravity model, since the estimated fixed effects modify the masses (assets and jobs) in each municipality to ensure adjustment. The use of fixed effects also prevents the coefficients from being estimated. $\alpha$ and $\beta$ and forces us to set *a priori* values. It is therefore possible to estimate a fixed effects model with $\alpha$ and $\beta$ zero, i.e. neutralising the effect of origin or destination masses and making flows depend only on the distances between communes.

\[\^7\] Random effects: Only the results of fixed-effect regressions (origin and destination municipalities) are reported in tables or graphs; random effects give very similar results. However, for out-of-sample predictions (*out of the bag*), we use random effects regressions, as the fixed effects are not known outside the sample.

In @tbl-gravcom, the standard deviations of the estimated coefficients are reported. They are calculated in two ways. The first line (in italics) is the standard deviation deduced from the model. For example, for line 1, this is the standard deviation for each coefficient for OLS, i.e. assuming that the residuals are normal. Below, the standard deviation estimated by resampling (*boostrap*) is calculated directly on the sample of coefficients estimated on the resampled observations. For OLS estimates, these two standard deviations differ greatly, which calls into question the assumption of normality of the residuals and the model chosen. On the other hand, for models estimated by `glm` and with a generative process following a Poisson distribution, the standard deviations are very close using both methods.

The @tbl-biasgrav shows the aggregation bias for each of the estimated models. The aggregation bias of the gravity model is due to the fact that $e^{E(log(\hat f_{ij}))} \neq E(\hat f_{ij}) \neq E(f_{ij})$. However, the gravity model, in its log-linear form, only guarantees the equality of the expectations of the $log$. It follows that, because of the convexity of the function $log$ function, there is a bias. The "total bias" column illustrates the extent of the difference between the sum of predicted flows and the sum of observed flows. This difference exceeds 60% for the simple gravity model and 150% for the constrained model. Weighting the flows in the regressions reduces the problem, but only the formulation in `glm` with a Poisson process reduces this bias to 0. In fact, with this model $E(\hat f_{ij}) = \hat f_{ij}$ which ensures the correct aggregation of the predicted flows.

```{r}
#| label: tbl-biaisgrav
#| tbl-cap: Modèles gravitaires, biais agrégé total, en ligne ou en colonne

bd_read("tbl_gravbiais_glm")
```

In the same way that we analyse the total aggregate bias, we can analyse what is happening in rows and columns. By aggregating the predicted outgoing flows for each municipality of residence, we obtain a quantity that can be compared with the observed number of working people. The proposed measure is to square the sum of the deviations for each row against the total number of workers. The same procedure is used for the columns. Constrained models imply a significant aggregate bias in both columns and rows, in line with the total aggregate bias. The use of fixed effects reduces the aggregation bias very close to 0, in total, in rows and in columns, as expected.

### Non-linear estimates

The non-linear approach is used to estimate the parameters of models that are more complex than those formulated linearly. In non-linear estimates, we look for parameters that minimise an objective function that is either the weighted sum of the differences between observed flows and predicted flows [@eq-msrew] or the relative Kullback Leibler entropy [@eq-llmulti].

Using this approach, we estimate two types of gravity model: a model constrained in rows and a model doubly constrained in rows and columns, specified using @eq-gravfurness. For each value $\delta$ of the distance parameter, we iteratively compute the $\hat a_i$ or the $\hat a_i$ and $\hat b_j$.

For *MEAPS* we define a *odds ratios,* for example $o_{ij} = O(d_{ij})$ and for each parameter value, the flows are calculated using the MEAPS algorithm. We then look for the parameters that minimise relative entropy.

For both the gravity and MEAPS models, the estimates are repeated on the resampled observations in order to calculate a distribution of all the statistics and coefficients determined at each stage. This makes it possible to calculate standard deviations for the estimated coefficients.

The results of these estimates for the gravity models are reported in the @tbl-gravnonlinear and in the @tbl-meapsnonlinear for the various*odds-ratios* of *MEAPS*. The structure of these tables is slightly different from those reporting results by MCO or `glm` since, for example, it is not possible to calculate the proportion of deviance explained.

```{r}
#| label: tbl-gravnonlineaire
#| tbl-cap: Estimations non linéaires commune à commune, modèles gravitaires
tbl_grav_nl <- bd_read("tbl_grav_nl")

tbl_grav_nl
```

The use of the Kullback Leibler relative entropy metric (*KL*) or weighted squared errors give close estimates. The gravity model with Furness gives the best estimation results and leads to a distance parameter very close to that obtained for the fixed-effect gravity model estimated by `glm` and a Poisson process. An important nuance is that the $\hat a_i$ and $\hat b_j$ are determined from observations of the number of workers and jobs per municipality and can therefore be projected out of the sample, as long as these margins are observed.

The MEAPS estimates are based solely on the metric *KL*. For reference, the flows from a MEAPS without parameters, and therefore calibrated solely on the basis of row and column margins. The predictive capacity of this model (line 1) is less good than parameter models (lines 2 to 5), but of the same order of magnitude as gravity models with no fixed or random effect [@tbl-gravcom]. An important difference with these models is that both the separability property and the absence of aggregation bias (total, row and column) are ensured by the construction of MEAPS.

Different functional forms are considered:

Line 2: one parameter for all diagonal terms, i.e. flows from a commune of residence to that same commune for employment. Formally, $\omicron_{i \neq j}=1$ and $\omicron_{ii} = o$. The estimated parameter is 1.06 with a standard error of 0.05.

Line 3: One parameter for all diagonal flows where the density is above a threshold and 1 everywhere else. This form therefore has two parameters and the estimated threshold is that the*odds ratio* is greater than 1 (1.17) for just over 35% of the population (the threshold applies to municipalities classified by increasing density, which account for more than 65%, with a standard error of 4 points). L'*odds-ratio* is thus higher than that of the line 2 model.

Line 4: One parameter for the diagonal on the one hand and the communes bordering the diagonal on the other (i.e. 2 parameters). Formally, $\omicron_{ii} = o_d$; $\omicron_{ij\in \mathcal{V}(i)} = o_v$ and $\omicron_{i, j \neq i, j \notin \mathcal{V}(i)} = 1$. This specification does not add much to line 1.

Lines 5 and 6: in these two specifications, the *odds ratios* depend on the distance between the commune of origin and the commune of destination, making it possible to combine *MEAPS* where it is the rank for each individual that differentiates opportunities and a distance-based approach. These specifications have two parameters and the dependence on distance is either linear (line 6) or exponential (line 5). These two specifications produce the best fits.

```{r}
#| label: tbl-meapsnonlineaire
#| tbl-cap: Estimations non linéaires commune à commune, MEAPS
tbl_meaps_nl <- bd_read("tbl_meaps_nl")
tbl_meaps_nl
```

Examination of the estimated observed graphs confirms what the metrics indicate [@fig-divmailles]. It should be noted that, apart from the gravity model constrained in line only, each of the models estimated by the non-linear procedure leads to a prediction close to the observed for the largest flows. The differentiation is then based on the ability to take into account flows of less than 1,000.

It should be remembered that although the gravity model modified by Furness has good predictive capacity (see also out-of-sample performance), this is to the detriment of the explanatory scope of this model. MEAPS has the immense advantage of understanding the mechanics at work, which lead to the constraints being respected in both rows and columns.

```{r}
#| label: fig-divmailles
#| fig-cap: Estimations non linéaires, Observés versus estimés
#| fig-asp: 1
library(scales)
datafm <-bd_read("datafm")

ggplot(datafm)+
  geom_point(aes(x=mobpro, y=flux,
                 color = nom0, size=sqrt(mobpro)), alpha=.5, 
             show.legend = FALSE, stroke = 0) +
  scale_size(range = c(0.25, 2.5))+
  scale_x_log10(name = "observé", limits=c(1, 60000), labels = c("1", "10", "1k", "50k"), breaks = c(1, 10, 1000, 50000))+
  scale_y_log10(name = "estimé", limits=c(1, 60000), labels = c("1", "10", "1k", "50k"), breaks = c(1, 10, 1000, 50000))+
  geom_abline(slope=1, linewidth = 0.1, color = "grey", linetype = "solid")+
  coord_equal()+
  facet_wrap(vars(nom0), nrow = 3, ncol = 3)+
  theme_ofce(base_size = 9, 
             strip.text = element_text(size = 8),
             panel.grid.major.x = element_line(color = "grey", linewidth = 0.1))+
  labs(caption = "*Sources* : MOBPRO, MEAPS")
```

### Out-of-sample performance

Out-of-sample testing of predictive models is a strong discipline that reveals many model properties. The @tbl-oob reports the metric $R^2_{KLi}$ of the model estimated for La Rochelle-Aunis simulated for the distances and masses (assets and jobs) observed in other SCoTs. For most of the SCoTs, the gravity models do less well than an independent distribution - which uses information on the margins. The models estimated using the non-linear procedure, on the other hand, systematically do better than the independent distribution and obtain scores comparable to those on the estimation sample. The information on the margins (the number of workers and jobs per municipality) ensures good predictive capacity, which is improved by the modelling since the hierarchy between the models is preserved.

```{r}
#| label: tbl-oob
#| tbl-cap: "Prédictions hors édhantillon (**out of the bag**)"

data_glm_oob <- bd_read("data_glm_oob") |> 
  mutate(type = "Estimations linéaires")
data_nl_oob <- bd_read("data_nl_oob") |> 
  mutate(type = "Estimations non linéaires")

bind_rows(data_glm_oob, data_nl_oob) |>
  group_by(type) |> 
  gt() |> 
  fmt_percent(columns = -c(id, nom), decimals = 1) |> 
  cols_hide(c(id, metrique, type)) |> 
  fmt_markdown(columns = metrique) |> 
  tab_spanner_delim(delim="_", columns = -c(id, nom)) |> 
  cols_label(nom="") |> 
  tab_source_note(md("La métrique reportée est le R^2^~KLi~, l'entropie relative KL ou gain d'information ; référence indépendante (n~i~ et e~j~ connus) pour les flux prédits à partir des distances de chaque territoire"))

```

## Adjustments using sub-municipal information

We have information at the 200m level, which may be relevant for reproducing the @MOBPRO data, even though the data is known between communes. In fact, we can locate jobs and residents more precisely, by tile, calculate journey times between pairs of tiles and feed this geographical information into the model. This can be expected to take better account of configurations, particularly for neighbouring municipalities. The distance between centroids can mask a high housing density on the border between two municipalities, and conversely, the bi-polar structure of a municipality and therefore the flows that are distributed between two close neighbours can be neglected. By using this geographical representation on a finer scale, we can propose more robust parameterisations with greater significance.

This approach generally poses a difficult algorithmic optimisation problem. A crude approach, which consists of minimising a loss function measuring the difference between estimated flows and observed flows, comes up against the large size of the parameter space. In addition, as is always the case in this type of statistical exercise, the challenge is to extract general information from the available data, leaving aside anything that is specific to a particular data set. This is the difficulty of overlearning (*overfitting*) that we mentioned earlier.

A second, more parsimonious approach is to define a functional form for the *odds ratios* or grouping the *odds ratios* into a few *clusters* and then evaluate only a small number of parameters. This involves modelling the structuring of the *odds ratios* based on*a priori* on the relevant dimensions.

### Sub-municipal data

#### Jobs, residents on the Inspire 200m tile {#sec-emplois}

The map of the area under consideration is shown at @fig-zoneslr. The analysis is limited to residents of the Schéma de COhérence Territoriale (SCOT) and considers jobs within a 33-kilometre radius of their place of residence. This map is based on gridded data from @C200 at the resolution of the Inspire 200m grid.\[\^8\]. We add to this data the location of jobs on the same grid, using land files and localised jobs data from @MOBPRO. The method consists of allocating jobs in each municipality by NAF code according to @MOBPRO to the professional surface areas per plot of land from the land files. This is then used to locate jobs on a 200m grid. This method is rather crude, since in particular the person/surface area ratio is not constant from one company to another, but it provides a good first approximation, especially as the extrapolation does not go beyond the commune level. In any case, it is far superior to a uniform allocation.

\[\^8\] INfrastructure for SPatial InfoRmation in Europe has been a directive for the production of spatial data since 2007. Inspire defines a grid and its harmonised projection system. This is what follows INSEE in the dissemination of gridded data. See<https://inspire-geoportal.ec.europa.eu> for the definition of the grid and data sets.

```{r}
#| label: fig-zoneslr
#| fig-scap: "Localisation des résidents et des emplois"
#| fig-cap: "Localisation des emplois et des résidents, zones de la Rochelle. Le périmètre de du SCOT de la Rochelle est indiqué ainsi que les limites administratives des communes et des EPCI le composant.<br>*Sources* : OSM, Mapbox, IGN, carroyage INSEE 2017, Flores et fichiers fonciers 2018"

knitr::include_graphics("output/popemp.png")
```

#### Calculating distances by mode {#sec-distancesparmode}

An important ingredient in the analysis of the region is the distances between each possible residence/employment pair. Unlike the synthetic analysis, we are not content with Euclidean distance.

To do this, we use a route calculator (R^5^ de Conveyal [@conway2017; @conway2018; @conway2019] using the package`{r5r}`[@r5r] distances and above all travel times for four modes (car, bicycle, public transport, walking). The travel times calculated for each pair of residence and employment squares, using the centre of the squares, take account of the various traffic constraints (speed limits for cars, direction of traffic, penalty for changing direction, authorised or restricted access depending on the mode, stress on bicycles). As far as car travel is concerned, we are not taking congestion into account at this stage. As far as public transport is concerned, the level of detail is quite high, since vehicle traffic frequencies and connections are taken into account. In some cities, it is possible to access information on actual journey times (thus measuring congestion or network availability) in addition to theoretical timetables. This information is not available for the La Rochelle conurbation, so this possibility is not being explored. Access to GTFS data has a number of limitations, such as not taking into account school networks or other local or private networks not published in this format. Changes to the transport network, such as the opening of a new line or an increase in frequency, are taken into account by modifying the matrix of distances and times by mode between each residence block and each destination block. In the case of the La Rochelle conurbation, the number of pairs calculated is around 16 million.

Based on the travel times by mode, we apply a discrete choice model, *Random Utility Model* (RUM), estimated on the basis of the @MOBPERS personal mobility survey and using @MOBPRO business mobility data to calibrate commune-to-commune flows. The estimation of this model is detailed in another document (reference to be inserted).

```{r access}
#| label: fig-accessibilite
#| fig-cap: "Temps d'accès à l'emploi, pour différents seuils. Pour chaque carreau de résidence, on détermine le temps minimal pour atteindre au moins 1 000, 5 000, 10 000, 20 000 emplois suivant l'un des quatre modes considéré.<br>*Source* : OSM, Mapbox, IGN, Conveyal R5, carroyage INSEE 2017, Flores et fichiers fonciers 2018"
#| fig-asp: 0.9

access <- qs::qread("output/acces4modes.sqs")
decor_carte <- bd_read("decor_carte")
if(is_html_output())
  accpanels <- set_names(c("to1k", "to5k", "to10k", "to20k")) else
  accpanels <- set_names(c("to10k"))  
  
access_4modes <- 
  map(accpanels, ~{ 
    ggplot()+
      decor_carte +
      ofce::theme_ofce_void(axis.text = element_blank()) +
      geom_sf(data=access, aes(fill=.data[[.x]]), col=NA)+
      PrettyCols::scale_fill_pretty_c(
        "Rainbow", 
        limits = c(0,100),
        breaks = c(15, 30, 45, 60, 75, 90),
        na.value = "gray85",
        direction=-1, 
        legend_title = glue("temps pour \n{str_remove(.x, 'to')} emp."))+
      annotation_scale(line_width = 0.2, height = unit(0.1, "cm"), 
                       text_cex = 0.4, pad_y = unit(0.1, "cm"))+
      facet_wrap(vars(mode))
  })
nacc <- names(access_4modes) |>
  str_remove("to") |> 
  str_remove("k") |>
  as.numeric() 
nacc <- format(nacc*1000, big.mark=" ")
if(is_html_output())
  {
  src <- map2(
    names(access_4modes), 
    nacc,
    function(x, y) str_c("#### ",
                         y,
                         " emplois\n", 
                         knit_expand(file = "_templates/access_template.qmd")))
  src <- c(":::{.panel-tabset}\n", src, "\n:::")
  } else {
    src <- map2(
    names(access_4modes), 
    nacc,
    function(x, y) knit_expand(file = "_templates/access_template.qmd"))
  }

acc_cr <- str_c(str_c("@fig-acc", accpanels), collapse = ", ")
```

The distances between each pair of boxes are used to calculate an accessibility indicator which plays a central role in the radiative model, and therefore in *MEAPS* by replacing the distance by the sum of opportunities below a time threshold. The @fig-accessibility maps represent the times taken to access a job threshold using different modes of transport.

::: {#fig-accessibilite}
`r knit(text = unlist(src))`

Access time to employment. For each residential district, the minimum time required to reach at least 1,000, 5,000, 10,000 or 20,000 jobs is determined according to one of the four modes considered.<br>Authors' calculations. <br>*Source* OSM, Mapbox, IGN, Conveyal R5, INSEE 2017 grid, Flores and land files 2018
:::

The @fig-comaccess accessibility curves are constructed by taking the average access time per commune of residence for the different employment thresholds. This curve is derived from the theoretical model presented elsewhere ([Theoretical aspects](theorie.qmd)) and which determines individual choices of commute and location. These curves reveal a property that is specific to coastal towns: while access to employment is greatest in La Rochelle for short commutes, other towns enjoy a more 'central' position when commuting times of more than 30 minutes by car are accepted.

```{r}
#| label: fig-comaccess
#| fig-scap: "Accessibilité par communes pour la Rochelle"
#| fig-cap: "Courbe du temps d'accès aux emplois. Pour chaque commune, on calcule la médianne, pondérée par le nombre d'habitants par carreau, du temps d'accès à différents seuils d'emplois. Cela permet de caractériser les communes par leur accessibilité à l'emploi, une mesure plus pertinente de la 'distance à l'emploi'.<br>*Sources* : OSM, Mapbox, IGN, Conveyal R5, carroyage INSEE 2017, Flores et fichiers fonciers 2018"

mode_l <- qs::qread("output/model_l.sqs")
library(scales)
ggplot(mode_l) +
  geom_line(aes(x=temps, y=emp, group=com21), col="gray80", linewidth=0.2) +
  geom_line(data = ~filter(.x, !str_detect(label, "^n")),
            aes(x=temps, y=emp, color=label)) +
  scale_x_continuous(breaks  = c(0, 20,40,60,80,100,120))+
  scale_y_continuous(labels = ofce::f2si2, breaks = c(25000, 50000, 75000, 100000))+
  PrettyCols::scale_color_pretty_d("Bold")+
  ofce::theme_ofce()+
  xlab("temps en minutes") +
  ylab("nombre d'emplois accessibles")+
  labs(color="Communes")+
  theme(legend.position = c(0.01, 0.99),
        legend.justification = c(0,1),
        panel.spacing = unit(12, "pt"),
        plot.margin = margin(l = 6, r= 6),
        panel.grid.major.x = element_line(color="gray80", linewidth = 0.1))+
  facet_wrap(vars(mode))

```

::: {#tip-ergodicite .callout-tip collapse="true"}
## Ergodicity

The @fig-reflr represents the $R^2_{KL}$ calculated for the reference model (MEAPS at 200m grid spacing) by running Monte-Carlo simulations for different sample sizes in order of priority. Unsurprisingly, the larger the sample, the more the distribution of $R^2_{KL}$ distribution. For 256 draws, the 95% confidence interval for the $R^2_{KL}$ is of the order of 0.017% (compared with 0.04% for 64 draws and 0.003% for 1024 draws), which will be sufficient for most applications.

The mean value of $R^2_{KL}$ value obtained for the reference MEAPS is 88.4%.

```{r}
#| label: fig-reflr
#| fig-cap: "Densité des $R^2_{KL}$ simulés par bootstrap pour une simulation de Monte-Carlo sur 64 ou 256 ou 1024 tirages."
#| fig-asp: 0.61

stats <- qs::qread("output/bootstrap r2kl.qs")
density <- map_dfr(c(64, 256, 1024), ~{
  dd <- density(stats |> filter(lbst==.x) |> pull(r2kl2))
  tibble(x  = dd$x, y=dd$y, lbst = .x)
  })

ggplot(density)+
  geom_area(aes(
    x=x, y = y,
    group=factor(lbst), col = factor(lbst), fill = factor(lbst)),
    alpha=0.66, position="identity")+
  scale_x_continuous(labels = scales::label_percent(.01))+
  scale_y_continuous(labels = scales::label_number(bug.mark="&nbsp"))+
  scale_color_brewer(palette="Accent", name = "Tirages M.C." , aesthetics = c("color", "fill"))+
  xlab(latex2exp::TeX("$R^2_{KL}$"))+ylab(NULL)+
  theme_ofce()
```
:::

### Saturated model: estimation of as many*odds ratios* as commune pairs {#sec-estnp}

At this stage, we use a naive algorithm to find a solution to the problem posed. We calculate the *odds ratios* $\omicron^k_{ij}$ that would make it possible to close the gap between the MEAPS forecasts made with a set of*odds ratios* $\omicron^{k-1}_{ij}$ and the observed data from @MOBPRO using the following formula where $\beta$ is a positive damping parameter less than 1 and where $k$ indexes the iterations:

$$
\omicron^k_{ij} = \biggl(\frac{\tilde{c}^k_{abs}}{
c^{mobpro}_{abs}}\biggr)^\beta \times \omicron^{k-1}_{ij}
$$ {#eq-algest}

We then modify the $\omicron_{ij}$ according to the differences observed. This leads us to look for a fixed point.

The naive algorithm is relatively efficient. It converges in a few dozen iterations, proves stable and reduces relative entropy. It will have to be refined in the future to allow a gradient descent that explicitly minimises relative entropy. The naive algorithm reduces this relative entropy without ensuring that it is minimal.

This algorithm has been used with different constraints on the parameters. The @tbl-meapsR2-np indicates the quality of the fit obtained in these different configurations. The first is where the absorption probabilities are determined solely by the leakage per commune of residence. This is the most parsimonious configuration in terms of parameters and is used as a reference. The $R^2_{KL}$ is 88%, which is a high level of adjustment. The second configuration is the one where we adjust the $\omicron_{ij}$ only for diagonal terms ($i=j$). This configuration adjusts a *odd-ratio* for residents who work in their commune of residence. In a number of municipalities, this adjustment increases the probability of internal absorption (@fig-carteodd), indicating that the choice of residence is not independent of the choice of activity. For the largest municipality (La Rochelle), on the other hand, the*odd-ratio* $\omicron_{17300, 17300}$ is close to 1. The next two configurations leave many more degrees of freedom when estimating $\omicron_{ij}$ freely. The first of these two configurations limits the $\omicron_{ij}$ estimates to those representing a cumulative total of flows measured by @MOBPRO equal to 99.4%, i.e. 1,854 $\omicron_{ij}$ . The second configuration estimates all the $\omicron_{ij}$ without limit (i.e. 2,033 parameters for 72 communes of residence and 210 communes of activity, with a large number of links not considered because they are zero).

```{r meapsR2-np}
#| label: tbl-meapsR2-np
#| tbl-cap: Ajustements non paramètriques, mobilités professionelles la Rochelle

meaps_stats <- qs::qread("output/meaps_stats.sqs") |> 
  arrange(r2kl)
meaps_stats |> 
  select(-f_in, -f_out, -p1, -p2) |> 
  filter(alg %in% c("référence", "diagonale", "90%", "99%", "100%")) |> 
  mutate( 
    alg = factor(alg, c("référence", "diagonale", "90%", "99%", "100%")),
    label = case_match(alg,
             "100%" ~ "100% des flux cumulés",
             "99%" ~ "99% des flux cumulés ",
             "90%" ~ "90% des flux cumulés",
             "diagonale" ~ "Diagonale (résidence égale emploi)",
             "référence" ~ "Référence (odds unitiaires)" )) |> 
  relocate(label) |> 
  gt() |> 
  fmt_percent(columns = r2kl, decimals = 1) |>
  fmt_integer(columns = c(dl, n_est), sep_mark = " ") |>
  cols_hide(alg) |> 
  cols_label(label = "",
             r2kl = md("R<sub>KL</sub><sup>2</sup>"),
             dl = "Degrés de liberté",
             n_est = "odds estimés") |> 
  tab_footnote(md("Le nombre de degrés de liberté est le nombre de paires de flux non nuls dans MOBPRO, moins les contraintes en ligne et en colonne, plus un puisqu'elles sont redondantes moins le nombre de paramètres estimés. Le nombre de degré de liberté est nul pour les configurations 99% et 100% arce que le nombre de paramètres estimés est supérieur au produit des linges et des colonnes moins les contraintes. Il y a bien plus de paramètres estimés pour la configuration 100%  que pour 99%. En conséquence, l'algorithme conduit à un résultat légèrement différent."))
```

the @fig-actvsfit-np represents the observed and estimated flows for the different configurations of the @tbl-meapsR2-np. Estimating only the $\omicron_{ii}$ diagonals, i.e. adjusting only the flows going from a municipality of residence to itself, already gives very good results by passing the $R^2_{KL}$ from 88% to 95% and visibly reducing the discrepancies between observed and estimated flows, as shown in the top two panels of @fig-actvsfit-np. Adding extra parameters doesn't make much difference, especially as the deviations for marginal flows aren't reduced that much. The limit of the naive algorithm appears here, since the fully saturated model does not fully adjust the distribution. Various details of the algorithm may explain this, in particular the censorship of the *odd-ratio* that are too low (\<0.0001) or too high (\>10000) or the inclusion of zero flows. Beyond this argument, it is likely that in order to converge towards a tighter fit, it would be necessary to calculate the matrix of quasi-derivatives of the flows with respect to the $\omicron_{ij}$.

But the cost could be very high, since this matrix (calculated in the synthetic section in a simple case) is of considerable size (1,755 \$\times\$1,755 coefficients), especially when you consider that calculating each term takes around twenty seconds.[^4].

[^4]: Around a year of vCPU...

```{r actvsfit-np, fig.asp=1}
#| label: fig-actvsfit-np
#| fig-scap: "*MEAPS* observés versus estimés"
#| fig-cap: "La figure présente pour chaque configuration d'estimation le flux observé (axe des x) et le flux estimé (axe des y) en bleu lorsque o<sub>i,j</sub> est estimé et en rouge lorsque o<sub>i,j</sub> n'est pas estimé (les fuites sont toujours utilisées). La valeur de référence est répétée dans chaque panneau en gris clair."

meaps_estimations <- qs::qread("output/meaps_est.sqs") |> 
  mutate(alg = factor(
    alg, c("référence", "diagonale", "90%", "99%", "100%",
           "gravitaire sans furness", "gravitaire avec furness",
           "un en diagonale", "2 en diagonale", 
           "un fonction distance", "distance critique")),
    grav = case_match(alg,
                      c("gravitaire sans furness", "gravitaire avec furness") ~ TRUE, 
                      .default = FALSE),
    np = case_match(alg,
                    c("diagonale", "90%", "99%", "100%") ~ TRUE,
                    .default = FALSE))

library(scales)
ref <- meaps_estimations |>
  filter(alg == "référence") |> 
  mutate(diag = COMMUNE==DCLT) |> 
  filter(flux!=0) |> 
  select(-alg)
non_param <- meaps_estimations |>
  filter(np) |> 
  mutate(diag = COMMUNE==DCLT) |> 
  filter(alg!="référence") |> 
  filter(flux!=0)
ggplot(non_param)+
  geom_point(data=ref,
             aes(x=mobpro, y=flux, shape = diag, alpha = diag, size = diag), 
             col="gray80")+
  scale_shape_manual(values=c(1, 18))+
  scale_alpha_manual(values=c(0.1, 0.5))+
  scale_size_manual(values=c(.5, 1.5))+
  geom_point(data = ~.x |> filter(!estime),
             aes(x=mobpro, y=flux, shape=diag, alpha = diag, size = diag),
             col="tomato")+
  geom_point(data = ~.x |> filter(estime),
             aes(x=mobpro, y=flux, shape=diag, alpha = diag, size = diag), 
             col="royalblue2")+
  scale_x_log10(limits=c(0.1, 20000),
                labels = label_number(accuracy = 1,
                                      big.mark = " "))+
  scale_y_log10(limits=c(0.1, 25000), 
                labels = label_number(accuracy = 1,
                                      big.mark = " "))+
  xlab("Flux observés")+ ylab("Flux estimés") +
  facet_wrap(vars(alg), ncol = 2)+
  geom_abline(slope=1, linewidth=0.25)+
  coord_equal()+
  ofce::theme_ofce(legend.position  = "none")
```

It should be noted that the mobility sample provided by @MOBPRO for the La Rochelle conurbation is very specific. One municipality (La Rochelle, with the geographical code 17300) accounts for almost 29% of mobility flows (from La Rochelle, the place of residence, to La Rochelle, the place of employment). This is therefore a monocentric pattern, where both residents and jobs are concentrated in a small area. The spatial resolution of @MOBPRO does not allow us to detail the more detailed structure.

For the 20 largest municipalities in the La Rochelle conurbation -- each of which has more than 1,000 active residents -- we can represent the *odds ratios* estimated in the 100% flows configuration can be plotted against the odds calculated in the case where all the $\omicron_{ij}$ are equal to 1 ( *odds ratios*) as a function of the distance between the commune of destination and the commune of residence\[\^10\]. This diagram, similar to a spectrum, can also be constructed for each destination municipality, with the distance$d$ is the distance to the various communes of residence @fig-spectreE. The most striking feature is that the *odds ratios* of $i$ à $i$ are generally greater than 1 (@fig-spectreR), with the exception of the commune of La Rochelle. No particular structure emerges in relation to distance, apart from the *odds ratios* ratios for large distances

\[\^10\] The distance is constructed as the weighted average distance between residents in the municipality of departure and jobs in the municipality of arrival. The weighting is the product of jobs and residents for each pair, normalised to 1.

```{r spectreR}
#| label: fig-spectreR
#| fig-scap: "Odd-ratio par commune de résidence fonction de la distance aux communes d'emploi (spectre résidents)"
#| fig-cap: "La figure représente pour les 20 plus grandes communes de l'agglomération de la Rochelle les odd-ratios estimés (configuration 100% des flux) en fonction de la distance entre cette commune et les communes où travaillent les résidents. Les points marqués d'un petit point blancs sont les emplois situés hors du périmètre du SCoT."
knitr::include_graphics("output/spectre effectif par COMMUNE 100.png")
```

```{r spectreE}
#| label: fig-spectreE
#| fig-scap: "Odd-ratio par commune d'emploi fonction de la distance aux communes de résidence (spectre emplois)"
#| fig-cap: "La figure représente pour les 20 plus grandes communes d'emplois du périmètre géographique (33 km autour de l'agglomération de la Rochelle) les odd-ratios estimés (configuration 100% des flux) en fonction de la distance entre cette commune et les communes où résident les travailleurs de la commune."

knitr::include_graphics("output/spectre effectif par DCLT 100.png")
```

The @fig-carteodd is used to specify the high value of the *odds ratios* for internal flows. Municipalities with a large number of jobs have a high odds ratio. *odds ratio* are estimated to be higher in smaller, less-serviced towns. For the different estimation procedures and therefore different numbers of estimated parameters, a similar structure can be observed in the geographical distribution of the odds ratios. *odds ratios* which suggests that *odds ratios* contain information.

A *odds ratio* in the diagonal indicates that internal flows are greater than in the reference scenario. This probably indicates a choice of residence linked to the job held, with the municipality of employment being preferred as the place of residence (or possibly vice versa). The spectrum of residents as a function of distance indicates that this phenomenon, while a hypothesis at very short distances, does not persist outside the municipality of residence. On the other hand, the @fig-spectreE suggests that in some communes, notably Surgères, there are *odds ratios* greater than 1 for short distances, which can be interpreted as the fact that residents of the surrounding communes prefer Surgères as their place of employment.

At this stage, observations are limited by the small number of municipalities modelled, but we can hope that the analysis of *odds ratios* can be used to characterise municipalities in terms of residential and employment choices. By multiplying this analysis for other areas, the information provided by the *odds ratios* can be inferred. It will also be possible to compare these elements with other variables, such as property prices, residential or commercial rents, and employment density.

```{r carteodd}
#| label: fig-carteodd
#| fig-scap: "Odd-ratio dans la diagonale"
#| fig-cap: "Chaque cercle indique les odd-ratio estimés dans la diagonale (100% des flux). Les diamètres des cercles sont proportionels aux flux internes (de i à i)."

knitr::include_graphics("output/toutes configs odds effectifs.png")
```

### Parametric estimates and comparison with the gravity model

Instead of directly estimating a set of*odds ratios* $\omicron_{ij}$ we can propose parametric functional forms from which we can calculate the *odds ratios*. This is a much more parsimonious strategy. The parameters of the chosen functional form are then determined using a standard algorithm for minimising relative entropy, which is the criterion we have chosen for comparing the distributions. It is also possible to conduct a parametric estimation for the gravity model.

Here we explore three functional forms for *MEAPS* :

1.  One parameter for all diagonal terms, i.e. flows from a municipality of residence to the same municipality for employment. This form is close to the "diagonal" form estimated in @sec-estnp, but only one parameter is estimated -- by a minimisation of the relative entropy -- instead of 72 by the iterative algorithm. Formally, $\omicron_{i \neq j}=1$ and $\omicron_{ii} = o$.

2.  A parameter for all diagonal terms and a parameter for neighbouring employment municipalities, i.e. a corrective term linking a municipality of residence to neighbouring municipalities. A municipality is close to another if at least 5% of journeys weighted by jobs and residents are less than 3 km apart. This definition makes it possible to exclude neighbouring municipalities whose main centres are far apart. Formally, $\omicron_{ii} = o_d$; $\omicron_{ij\in \mathcal{V}(i)} = o_v$ and $\omicron_{i, j \neq i, j \notin \mathcal{V}(i)} = 1$.

3.  A coefficient for the distance and a parameter for the "toggle" distance. Formally, below a distance $d_c$ a $\omicron_{ij \in d_{i,j} \leq d_c} = o$ and $\omicron_{ij \in d_{i,j} > d_c} = 1$. This form shares the same idea as the first model, but considers the notion of proximity instead of relying on the administrative division.

Each of these options measures an intra-municipal bias that can be explained by a joint choice of residence and employment location. *MEAPS*offers the possibility of measuring the intensity of this phenomenon in relation to the hypothesis where jobs are considered independently of location and are all perfectly substitutable. It will be interesting to compare territories from this point of view and to identify and quantify local specificities, whether they relate to the geography of the territory -- its structure in poles or satellites -- the formation of property prices, the transport network or the nature of economic activity. We could also seek to exploit sectoral information -- available in @MOBPRO at the level of 5 sectors -- or social or demographic information -- available at the municipal or IRIS level but which can also be exploited at a finer level with Fidéli.\[\^11\].

\[\^11\] Demographic files on dwellings and individuals, INSEE,<https://www.insee.fr/fr/metadonnees/source/serie/s1019>.

To these functional forms for *MEAPS* we are adding two functional forms for the gravity model:

4.  a gravity model following the @eq-gravity definition where $f(d)= e^{d/\delta}$. A single parameter $\delta$ is estimated.
5.  a "balanced" gravity model using the Furness algorithm, as described above, and estimating $\delta$ as in point 4.

We could multiply the estimated models[^5]. The aim here is to illustrate the possibilities of our modelling and to compare them with those of the gravity model. Two points stand out:

[^5]: For example, by making*odd-ratios* depend not on distance and a critical distance, but on rank and a critical rank.

-   *MEAPS* can better reproduce the data, with a better quality of fit,

-   *MEAPS* opens up richer possibilities for interpretation than the gravity model, because the microscopic foundations of *MEAPS* are explicit.

::: {#tip-emiettage .callout-tip collapse="true"}
## Crumbling

In the synthetic simulations presented in the document "[Theoretical aspects](theorie.qmd)"flows are simulated at an individual granularity. Each job or each individual is located and the distances are calculated between these locations and the flows per individual are simulated. Spatial aggregation to a hexagonal grid is then carried out. In the case of the data we are using for La Rochelle, the squares are not occupied by a single active resident or a single job. There are packets for which it is not necessary to repeat the simulations on an individual-by-individual or job-by-job basis. We have therefore grouped them together and simulated them accordingly in MEAPS. This poses a problem, however, since the choice of an order of priority is now made for individuals in packets of different sizes, with a small number of these packets being much larger than the median of the others. So when a large packet is its turn to choose, it can saturate jobs in a single pass. To solve this problem, we perform a crumble in which the largest packets are divided into smaller packets. For a crumbling threshold of 20 individuals (the largest @MOBPRO flow for La Rochelle is 18,000), we increase the number of packets by around 50%, which makes it possible to maintain a reasonable overall size while reducing the packet granularity problem. In addition, the packets are drawn at random in their order of priority, taking into account their size in order to avoid over-representation of small packets in the order of priority.
:::

Table @tbl-meapsR2-p summarises the estimation results. The benchmark model, in which all jobs are substitutable for each individual, does less well in terms of adjustment than the other models, with the notable exception of the unbalanced gravity model. As we saw in the non-parametric estimates, despite its simplifying hypothesis, the reference model performs well, and this is confirmed here by the comparison with the simple gravity model.

```{r meapsR2-p}
#| label: tbl-meapsR2-p
#| tbl-cap: Ajustements paramètriques, mobilités professionelles la Rochelle

meaps_stats_p <- qs::qread("output/meaps_stats.sqs") |> 
  arrange(r2kl) |> 
  select(-f_in, -f_out) |> 
  filter(alg %in% c("référence", "gravitaire avec furness", "gravitaire sans furness",
                    "un en diagonale", "2 en diagonale", "distance critique")) |> 
  mutate(
    labelp = case_match(alg,
             c("gravitaire avec furness", "gravitaire sans furness") ~ 
                 str_c("\u03B4\u2248", signif(p1, 2), " min"),
             "référence" ~ "",
             "un en diagonale" ~ str_c("o\u2248", signif(p1, 2)),
             "2 en diagonale" ~ str_c("o<sub>d</sub>\u2248", signif(p1, 2), "<br>",
                                               " o<sub>v</sub>\u2248", signif(p2, 2)),
             "distance critique" ~ 
               str_c("d<sub>c</sub>\u2248 ", signif(p1, 2)," min<br>",
                                               " o\u2248", signif(p2, 2))),
    label = case_match(alg,
             "gravitaire avec furness" ~ "5. Gravitaire avec Furness",
             "gravitaire sans furness" ~ "4. Gravitaire sans Furness",
             "référence" ~ " Référence",
             "un en diagonale" ~ "1. Commune vers commune",
             "2 en diagonale" ~ "2. Commune vers commune et voisines",
             "distance critique" ~ "3. Distance carreau 200m")) |> 
  arrange(label) |> 
  relocate(label) |> 
  select(-p1,-p2, -alg, -n_est)
meaps_stats_p |> 
  relocate(label) |> 
  gt() |> 
  fmt_percent(columns = r2kl, decimals = 1) |>
  fmt_integer(columns = c(dl), sep_mark = " ") |>
  fmt_markdown(columns = labelp) |> 
  tab_style(
    style = cell_borders(sides = "top", col="gray66"),
    locations = cells_body(rows = label == "4. Gravitaire sans Furness")) |> 
  cols_label(label = "",
             labelp = "Paramètres",
             r2kl = md("R<sub>KL</sub><sup>2</sup>"),
             dl = "Degrés de liberté") |>
  cols_align(columns = c(r2kl, dl, labelp), align= "center" ) |> 
  tab_footnote(md("Le nombre de degrés de liberté est le nombre de paires de flux non nuls dans MOBPRO, moins les contraintes en ligne et en colonne, plus un puisqu'elles sont redondantes moins le nombre de paramètres estimés. Les unités sont des minutes de trajet pour les paramètres homogènes à une distance et sans unité pour les *odd-ratios*."))
```

The estimates of models 1 to 3, in which a diagonal term is explored in different forms, reinforce the diagnosis of communal bias noted in the non-parametric estimates. There is on average 4 times more chance of choosing a job (@tbl-meapsR2-p, lines 1 and 2) in the municipality of residence. The estimation of model 2 shows that the neighbouring municipalities do not have a comparable bias, although the chance of choosing a job in these municipalities is greater than 1.

The estimation of model 3 indicates that distance apparently explains the municipal bias better than the administrative division, and it is more appropriate to see it as a proximity bias. Indeed, the adjustment coefficient is more than one point higher than that obtained with the first model, losing only 1 degree of freedom. The tipping distance is small, around 9 minutes, which suggests that the municipal perimeter is too large to capture this effect. The chance at the shortest distance is also significantly higher, since instead of being approximately 4 it is approximately 19, i.e. more than 4 times higher.

This estimate should be treated with caution at this stage, since the resolution of the data is well below the threshold that was found. The simulation is based on distances and job locations in the 200m grid, which are convincingly accurate. But the flows in @MOBPRO are only known for the municipalities of origin and departure, and therefore with a lower spatial resolution. Increasing the number of observations could compensate for this low spatial resolution, but this would require an analysis of distances and locations over larger and more numerous areas. To make progress, we would need to use more finely localised flow data, for example from Fidéli.\[\^13\] or digital tracking data.

\[\^13\] From Fidéli, we can specify the location of each individual and use the information on the municipality in which they work. However, it is not possible to pinpoint the location of the job held.

```{r actvsfit-p, fig.asp=1}
#| label: fig-actvsfit-p
#| fig-scap: "*MEAPS* observés versus estimés, estimations paramétriques"
#| fig-cap: "La figure présente pour chaque configuration d'estimation le flux observé (axe des x) et le flux estimé (axe des y) en bleu lorsque o<sub>i,j</sub> est estimé et en rouge lorsque o<sub>i,j</sub> n'est pas estimé (les fuites sont toujours utilisées). La valeur de référence est répétée dans chaque panneau en gris clair."

meaps_estimations <- qs::qread("output/meaps_est.sqs") |> 
  mutate(alg = factor(
    alg, c("référence", "diagonale", "90%", "99%", "100%",
           "gravitaire sans furness", "gravitaire avec furness",
           "un en diagonale", "2 en diagonale", 
           "un fonction distance", "distance critique")),
    grav = case_match(alg,
                      c("gravitaire sans furness", "gravitaire avec furness") ~ TRUE, 
                      .default = FALSE),
    np = case_match(alg,
                    c("diagonale", "90%", "99%", "100%") ~ TRUE,
                    .default = FALSE))
library(scales)
ref <- meaps_estimations |>
  filter(alg == "référence") |> 
  mutate(diag = COMMUNE==DCLT) |> 
  filter(flux!=0) |> 
  select(-alg)
param <- meaps_estimations |>
  filter(!np&!grav) |> 
  filter(alg != "un fonction distance") |> 
  mutate(diag = COMMUNE==DCLT) |> 
  filter(alg!="référence") |> 
  filter(flux!=0) |> 
  mutate(
  label = case_match(alg,
             "gravitaire avec furness" ~ "5. gravitaire avec Furness",
             "gravitaire sans furness" ~ "4. gravitaire sans Furness",
             "référence" ~ " MEAPS odds=1",
             "un en diagonale" ~ "1. Commune vers commune",
             "2 en diagonale" ~ "2. Commune vers commune et voisines",
             "distance critique" ~ "3. Distance carreau 200m")) |> 
  select(-alg)

ggplot(param)+
  geom_point(data=ref,
             aes(x=mobpro, y=flux, shape = diag, alpha = diag, size = diag), 
             col="gray80")+
  scale_shape_manual(values=c(1, 18))+
  scale_alpha_manual(values=c(0.1, 0.5))+
  scale_size_manual(values=c(.5, 1.5))+
  geom_point(data = ~.x |> filter(!estime),
             aes(x=mobpro, y=flux, shape=diag, alpha = diag, size = diag),
             col="tomato")+
  geom_point(data = ~.x |> filter(estime),
             aes(x=mobpro, y=flux, shape=diag, alpha = diag, size = diag), 
             col="royalblue2")+
  scale_x_log10(limits=c(0.1, 20000),
                labels = label_number(accuracy = 1,
                                      big.mark = " "))+
  scale_y_log10(limits=c(0.1, 25000), 
                labels = label_number(accuracy = 1,
                                      big.mark = " "))+
  xlab("Flux observés")+ ylab("Flux estimés") +
  facet_wrap(vars(label), ncol = 2)+
  geom_abline(slope=1, linewidth=0.25)+
  coord_equal()+
  ofce::theme_ofce(legend.position  = "none")
```

Parametric estimates indicate that the gravity model performs less well. If the column constraints are not respected, the gravity model gives a fairly distorted picture of journeys. It struggles to reproduce the proximity bias and the influence of distance. The former tends to produce a parameter $\delta$ parameter, whereas the second should impose a very high $\delta$ to account for longer distances. The application of the same distance value in more or less dense environments handicaps this representation. The Furness procedure improves the capacity of the gravity model to account for the data, but, as we said, at the cost of losing the link with distance as it is formulated in the gravity model, i.e. homogeneous for all.

The @fig-actvsfit-grav illustrates what is at work in the gravity model. Minimising relative entropy depends very much on flows within La Rochelle, which account for 29% of the sample. The other diagonal municipalities are not well taken into account, which leads to a $R^2_{KL}$ worse than the reference of *MEAPS* reference (all jobs are identical for each individual and differ only in their location). Compliance with the column constraint using the Furness procedure allows better account to be taken of diagonal municipalities (whose weight is 35% in the La Rochelle sample), but is not as good as the models *MEAPS* parametric or non-parametric models.

```{r actvsfit-grav, fig.asp=0.55}
#| label: fig-actvsfit-grav
#| fig-scap: "*MEAPS* observés versus estimés"
#| fig-cap: "La figure présente pour chaque configuration d'estimation le flux observé (axe des x) et le flux estimé (axe des y) en bleu lorsque o<sub>i,j</sub> est estimé et en rouge lorsque o<sub>i,j</sub> n'est pas estimé (les fuites sont toujours utilisées). La valeur de référence est répétée dans chaque panneau en gris clair."

meaps_estimations <- qs::qread("output/meaps_est.sqs") |> 
  mutate(alg = factor(
    alg, c("référence", "diagonale", "90%", "99%", "100%",
           "gravitaire sans furness", "gravitaire avec furness",
           "un en diagonale", "2 en diagonale", 
           "un fonction distance", "distance critique")),
    grav = case_match(alg,
                      c("gravitaire sans furness", "gravitaire avec furness") ~ TRUE, 
                      .default = FALSE),
    np = case_match(alg,
                    c("diagonale", "90%", "99%", "100%") ~ TRUE,
                    .default = FALSE))
library(scales)
ref <- meaps_estimations |>
  filter(alg == "référence") |> 
  mutate(diag = COMMUNE==DCLT) |> 
  filter(flux!=0) |> 
  select(-alg)
param <- meaps_estimations |>
  filter(grav) |> 
  filter(alg != "un fonction distance") |> 
  mutate(diag = COMMUNE==DCLT) |> 
  filter(alg!="référence") |> 
  filter(flux!=0) |> 
  mutate(
  label = case_match(alg,
             "gravitaire avec furness" ~ "5. Gravitaire avec Furness",
             "gravitaire sans furness" ~ "4. Gravitaire sans Furness",
             "référence" ~ " Référence",
             "un en diagonale" ~ "1. Un paramètre commune vers commune",
             "2 en diagonale" ~ "2. commune vers commune et voisines",
             "distance critique" ~ "3. Distance (odds et distance critique)")) |> 
  select(-alg)

ggplot(param)+
  geom_point(data=ref,
             aes(x=mobpro, y=flux, shape = diag, alpha = diag, size = diag), 
             col="gray80")+
  scale_shape_manual(values=c(1, 18))+
  scale_alpha_manual(values=c(0.1, 0.5))+
  scale_size_manual(values=c(.5, 1.5))+
  geom_point(data = ~.x |> filter(!estime),
             aes(x=mobpro, y=flux, shape=diag, alpha = diag, size = diag),
             col="tomato")+
  geom_point(data = ~.x |> filter(estime),
             aes(x=mobpro, y=flux, shape=diag, alpha = diag, size = diag), 
             col="royalblue2")+
  scale_x_log10(limits=c(0.1, 20000),
                labels = label_number(accuracy = 1,
                                      big.mark = " "))+
  scale_y_log10(limits=c(0.1, 25000), 
                labels = label_number(accuracy = 1,
                                      big.mark = " "))+
  xlab("Flux observés")+ ylab("Flux estimés") +
  facet_wrap(vars(label), ncol = 2)+
  geom_abline(slope=1, linewidth=0.25)+
  coord_equal()+
  ofce::theme_ofce(legend.position  = "none")
```

The @fig-distrdist confirms this diagnosis. It compares the cumulative distribution as a function of the weighted distance per kilometre between each municipality for different estimates, with the @MOBPRO flows used as a reference. The performance of the models is comparable for short distances (i.e. from the commune of La Rochelle to itself). The gravity model, with or without Furness, fails for intermediate distances and gives too much weight to very long distances. Parametric estimates based on *MEAPS* do a good job of reproducing the cumulative distribution of distances, particularly parametric model 3, which uses the distance to the 200m tile as its functional form.

```{r}
#| label: fig-distrdist
#| fig-scap: Distributions empiriques cumulées des distances
#| fig-cap: Distributions empiriques cumulées des flux selon la distance. MOBPRO est indiqué en trait pointillé noir. La figure du haut est la distribution cumulée, celle du bas la différence entre la distribution et celle de MOBPRO
load("output/dist.com.srda")
meaps_estimations <- qs::qread("output/meaps_est.sqs") |> 
  mutate(alg = factor(
    alg, c("référence", "diagonale", "90%", "99%", "100%",
           "gravitaire sans furness", "gravitaire avec furness",
           "un en diagonale", "2 en diagonale", 
           "un fonction distance", "distance critique")),
    grav = case_match(alg,
                      c("gravitaire sans furness", "gravitaire avec furness") ~ TRUE, 
                      .default = FALSE),
    np = case_match(alg,
                    c("diagonale", "90%", "99%", "100%") ~ TRUE,
                    .default = FALSE)) |> 
  filter(!is.na(alg)) |> 
  filter(!np|alg=="référence") 
distances <- qs::qread("output/meaps_est.sqs") |>
  group_by(COMMUNE, DCLT) |>
  summarize(d = first(d[!is.na(d)]),
            t = first(t[!is.na(t)]),
            mobpro = first(mobpro[!is.na(mobpro)])) |> 
  ungroup() |> 
  drop_na(d, t)
algs <- distinct(meaps_estimations, alg) |> pull(alg)
param <- map_dfr(algs, ~{
  meaps_estimations |> 
    filter(alg==.x) |> 
    select(-c(d, d5, d95, mobpro, t, t5, t95)) |> 
    right_join(distances, by=c("COMMUNE", "DCLT")) |> 
    mutate(flux = replace_na(flux, 0),
           alg = .x)
}) |> 
  mutate(
    label = case_match(alg,
                       "gravitaire avec furness" ~ "5. Gravitaire avec Furness",
                       "gravitaire sans furness" ~ "4. Gravitaire sans Furness",
                       "référence" ~ "0. MEAPS odds=1",
                       "un en diagonale" ~ "1. Commune vers commune",
                       "2 en diagonale" ~ "2. Commune vers commune et voisines",
                       "distance critique" ~ "3. Distance carreau 200m"))

param <- param |> 
  filter(!is.na(label)) |>
  group_by(label) |> 
  arrange(label, t) |> 
  mutate(
    cumflux = cumsum(flux)/sum(flux),
    cumpro  = cumsum(mobpro)/sum(mobpro)) |>
  ungroup()

bas <- ggplot(param)+
  geom_step(aes(x=t, y=cumflux-cumpro , col=label), linewidth = 0.25) +
  geom_step(
    data=~filter(.x, alg==algs[[1]]),
    aes(x=t, y=0), linetype="dashed", 
    position = "identity", na.rm=TRUE)+
  scale_color_brewer(palette="Set1")+
  theme_ofce(legend.position = "none",
             plot.margin = margin())+
  ylab("Ecart avec MOBPRO")+
  xlab(NULL)+
  scale_y_continuous(
    labels = scales::label_percent(1))+
  scale_x_continuous(
    limits=c(0, 90),
    labels = scales::label_number(scale=1, suffix=" min"),
    n.breaks = 8)
  
haut <- ggplot(param)+
  geom_step(
    aes(x=t, y=cumflux, col=label),
    position = "identity", na.rm=TRUE, alpha=1, linewidth = 0.25)+
  geom_step(
    data=~filter(.x, alg==algs[[1]]),
    aes(x=t, y=cumpro), linetype="dashed", 
    position = "identity", na.rm=TRUE)+
  scale_color_brewer(palette="Set1")+
  theme_ofce(
    legend.position = c(0.75, 0.3))+
  scale_x_continuous(
    limits=c(0, 90))+
  scale_y_continuous(
    labels = scales::label_percent(1),
    name = "Distribution cumulée le long de la distance")+
  theme(axis.line.x = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        plot.margin = margin(b=1))+
  guides(color=guide_legend("Estimation"))+
  xlab(NULL)+ylab(NULL) 

library(patchwork)
haut / bas + plot_layout(heights = c(3,1))
```

## Conclusion

The estimates presented here lead to several important results:

1.  a weighted or relative entropy metric produces more robust, more convincing results and much better predictive ability than the implicit (unweighted) ordinary least squares metric ;

2.  the use of additional data on the geography of the area (location of individuals, residents, transport networks) increases the quality of the estimates and the predictive capacity;

3.  MEAPS flow modelling has better properties and greater predictive capacity than the gravity model, even when the latter is estimated using a suitable metric and detailed geographical information, as long as parameters are introduced into the radiative model. The universal, parameter-free radiative model produces a correct result, but the fit is inferior to that of parameterised gravity models;

4.  the parameters of the gravity model are difficult to interpret. They depend on the specific spatial configuration and the scale of observation. MEAPS provides a structural approach with well-defined foundations, giving the parameters a more general meaning.

## Bibliographical references {.unnumbered}

::: {#refs}
:::
